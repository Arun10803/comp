{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03b3c743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverted Index:\n",
      "fox -> Document 1\n",
      ". -> Document 1, Document 2\n",
      "sun -> Document 2\n",
      "the -> Document 1, Document 2\n",
      "jumped -> Document 1\n",
      "quick -> Document 1\n",
      "dog -> Document 1, Document 2\n",
      "lazy -> Document 1, Document 2\n",
      "brown -> Document 1\n",
      "in -> Document 2\n",
      "slept -> Document 2\n",
      "over -> Document 1\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter your search query:  in\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Documents matching the query:\n",
      "Document 2\n"
     ]
    }
   ],
   "source": [
    "# Define the documents\n",
    "document1 = \"The quick brown fox jumped over the lazy dog .\"\n",
    "document2 = \"The lazy dog slept in the sun .\"\n",
    "\n",
    "# Step 1: Tokenize the documents\n",
    "# Convert each document to lowercase and split it into words\n",
    "tokens1 = document1.lower().split()\n",
    "tokens2 = document2.lower().split()\n",
    "\n",
    "# Combine the tokens into a list of unique terms\n",
    "terms = list(set(tokens1 + tokens2))\n",
    "\n",
    "# Step 2: Build the inverted index\n",
    "# Create an empty dictionary to store the inverted index\n",
    "inverted_index = {}\n",
    "\n",
    "# For each term, find the documents that contain it\n",
    "for term in terms:\n",
    "    documents = []\n",
    "    if term in tokens1:\n",
    "        documents.append(\"Document 1\")\n",
    "    if term in tokens2:\n",
    "        documents.append(\"Document 2\")\n",
    "    inverted_index[term] = documents\n",
    "\n",
    "# Step 3: Print the inverted index\n",
    "print(\"Inverted Index:\")\n",
    "for term, documents in inverted_index.items():\n",
    "    print(term, \"->\", \", \".join(documents))\n",
    "\n",
    "# Step 4: Search Query\n",
    "query = input(\"\\nEnter your search query: \").lower()  # Get the search query from the user\n",
    "query_terms = query.split()  # Split query into individual terms\n",
    "\n",
    "# Find the documents for the query\n",
    "result_docs = set()  # To store the matching documents\n",
    "\n",
    "# Iterate over the query terms and retrieve documents\n",
    "for term in query_terms:\n",
    "    if term in inverted_index:\n",
    "        result_docs.update(inverted_index[term])  # Add documents that contain the query term\n",
    "\n",
    "# Step 5: Display the results\n",
    "if result_docs:\n",
    "    print(\"\\nDocuments matching the query:\")\n",
    "    for doc in result_docs:\n",
    "        print(doc)\n",
    "else:\n",
    "    print(\"\\nNo documents found for the query.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63dcbb8",
   "metadata": {},
   "source": [
    "# Explanation:\n",
    "1. Inverted Index Construction: The inverted index is created the same way as before, mapping each word to the documents it appears in.\n",
    "2. Search Input: We prompt the user to input a search query.\n",
    "\n",
    "      -The query is converted to lowercase and split into individual terms.\n",
    "3. Search Query Processing: We search the inverted index for each query term and find which documents contain it.\n",
    "4. Result Display: If any matching documents are found, they are displayed. Otherwise, a message indicates no matches were found.# \n",
    "\n",
    "# How it Works:\n",
    "- Inverted Index: The program builds an index of words (terms) and the documents where those words appear.\n",
    "- Search: The user enters a search query, and the program checks which documents contain all the words in the query.\n",
    "- Results: The documents containing all the terms from the query are displayed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374bc821",
   "metadata": {},
   "source": [
    "# Alternative short\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19cdf7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog -> Document 2\n",
      "the -> Document 1, Document 2\n",
      "brown -> Document 1\n",
      "slept -> Document 2\n",
      "in -> Document 2\n",
      "fox -> Document 1\n",
      "lazy -> Document 1, Document 2\n",
      "sun. -> Document 2\n",
      "over -> Document 1\n",
      "jumped -> Document 1\n",
      "quick -> Document 1\n",
      "dog. -> Document 1\n"
     ]
    }
   ],
   "source": [
    "# Define the documents\n",
    "document1 = \"The quick brown fox jumped over the lazy dog.\"\n",
    "document2 = \"The lazy dog slept in the sun.\"\n",
    "\n",
    "# Step 1: Tokenize the documents\n",
    "# Convert each document to lowercase and split it into words\n",
    "tokens1 = document1.lower().split()\n",
    "tokens2 = document2.lower().split()\n",
    "\n",
    "# Combine the tokens into a list of unique terms\n",
    "terms = list(set(tokens1 + tokens2))\n",
    "\n",
    "# Step 2: Build the inverted index\n",
    "# Create an empty dictionary to store the inverted index\n",
    "inverted_index = {}\n",
    "\n",
    "# For each term, find the documents that contain it\n",
    "for term in terms:\n",
    "\tdocuments = []\n",
    "\tif term in tokens1:\n",
    "\t\tdocuments.append(\"Document 1\")\n",
    "\tif term in tokens2:\n",
    "\t\tdocuments.append(\"Document 2\")\n",
    "\tinverted_index[term] = documents\n",
    "\n",
    "# Step 3: Print the inverted index\n",
    "for term, documents in inverted_index.items():\n",
    "\tprint(term, \"->\", \", \".join(documents))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9bc0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Letâ€™s break down the code and explain each technical term and the entire procedure in detail.\n",
    "\n",
    "1. Tokenization\n",
    "Definition: Tokenization is the process of breaking a string of text into smaller units, typically words or phrases, called tokens.\n",
    "Why it's important: Tokenization is the first step in most text processing tasks because it helps convert raw text into structured data (tokens) that can be analyzed.\n",
    "In the code, tokens1 = document1.lower().split() splits the input document1 into a list of individual words by:\n",
    "\n",
    "Converting the text to lowercase using .lower() to ensure case insensitivity.\n",
    "Using .split() to break the text into words based on spaces.\n",
    "Similarly, tokens2 = document2.lower().split() does the same for the second document.\n",
    "\n",
    "2. Inverted Index\n",
    "Definition: An inverted index is a data structure used to store a mapping from words (terms) to their occurrences in a set of documents. It's called \"inverted\" because it flips the relationship between documents and terms: instead of storing which terms appear in a document, we store which documents contain a specific term.\n",
    "Why it's important: The inverted index is central to information retrieval systems like search engines because it allows for fast and efficient searching. When you search for a word, the inverted index immediately tells you which documents contain that word.\n",
    "In the code:\n",
    "\n",
    "The terms list is created by combining the unique tokens from both document1 and document2. This list contains all the distinct words in both documents.\n",
    "For each term, we then check whether it appears in each document and record which document(s) the term appears in. This is stored in the inverted_index dictionary, where the keys are terms and the values are lists of documents containing the term.\n",
    "Example:\n",
    "\n",
    "If the term \"lazy\" appears in both document1 and document2, the inverted index entry would look like:\n",
    "python\n",
    "Copy code\n",
    "inverted_index['lazy'] = ['Document 1', 'Document 2']\n",
    "3. Stopwords\n",
    "Definition: Stopwords are commonly used words (like \"the\", \"is\", \"in\", etc.) that don't carry much meaningful information in the context of text analysis. Removing stopwords is a common step in text preprocessing because they can introduce noise in text analysis tasks.\n",
    "Why it's important: Removing stopwords helps reduce the size of the dataset, focusing only on the meaningful terms that are relevant for tasks like information retrieval or text classification.\n",
    "In the code, the stopwords are removed from both the documents and the search query using the stopwords.words('english') list provided by the NLTK (Natural Language Toolkit). The code filters out any word in the document or query that appears in the stopwords list.\n",
    "\n",
    "Example:\n",
    "\n",
    "If \"the\" appears in a document, it will be ignored when building the inverted index or when processing a query.\n",
    "4. Preprocessing the Text\n",
    "Definition: Preprocessing is the step where raw text is cleaned and normalized before analysis. Common preprocessing steps include converting text to lowercase, removing punctuation, tokenizing, and removing stopwords.\n",
    "Why it's important: Preprocessing ensures that the text is in a consistent format, making it easier to work with and reducing variations in words (e.g., \"dog\" and \"Dog\" should be treated the same).\n",
    "The preprocess_text function in the code performs:\n",
    "\n",
    "Lowercasing: text = text.lower() converts the text to lowercase, which helps with case-insensitive matching.\n",
    "Punctuation Removal: text = text.translate(str.maketrans(\"\", \"\", string.punctuation)) removes punctuation (e.g., commas, periods, etc.) to avoid treating punctuation as part of the words.\n",
    "Stopword Removal: A list comprehension filters out stopwords from the tokens.\n",
    "5. Query Processing\n",
    "Definition: Query processing refers to how a user's search query is handled, from parsing the query to identifying the relevant documents that match the query terms.\n",
    "Why it's important: The query is typically processed in the same way as the documents to ensure consistency. For example, if both documents and the query are converted to lowercase and stopwords are removed, the search will be accurate even if the user enters the query in a different case or includes unnecessary words.\n",
    "In the code:\n",
    "\n",
    "The query is first converted to lowercase using query = input().lower().\n",
    "The query is then split into individual words (tokens) using query_terms = query.split().\n",
    "Stopwords are removed from the query using the preprocess_text function, which ensures that common words (like \"the\", \"in\") don't affect the search results.\n",
    "After processing the query, the terms are checked against the inverted index, and the relevant documents are retrieved.\n",
    "\n",
    "6. Search Results (Intersection of Documents)\n",
    "Definition: When a user submits a query, the system must identify which documents contain all the terms in the query. This is done by looking up each query term in the inverted index and collecting the documents that contain those terms.\n",
    "Why it's important: This is the heart of the search functionality: retrieving documents based on the user's query.\n",
    "In the code:\n",
    "\n",
    "For each term in the query (query_terms), the code checks if the term exists in the inverted index (if term in inverted_index).\n",
    "If the term is found, the corresponding documents are added to the result_docs set using result_docs.update(inverted_index[term]). Using a set ensures that duplicate document entries are avoided.\n",
    "Finally, the code prints the documents that match the query terms. If no documents match, it prints a message indicating no results were found.\n",
    "\n",
    "7. Set Operations\n",
    "Definition: A set is a data structure that stores unique elements. In Python, sets provide efficient membership tests and operations like union, intersection, and difference.\n",
    "Why it's important: Using a set to collect matching documents avoids duplicates, ensuring that each document is listed only once in the search results.\n",
    "In the code, the search results are stored in a set (result_docs), and the update() method is used to add documents that contain the query terms. This ensures that documents are only listed once, even if a term appears in multiple documents.\n",
    "\n",
    "Overall Procedure\n",
    "Tokenization: Split each document into words (tokens).\n",
    "Preprocessing: Clean and normalize the text by lowercasing it, removing punctuation, and removing stopwords.\n",
    "Inverted Index Construction: Build an inverted index that maps each unique word (term) to the documents where it appears.\n",
    "Query Processing: Accept a query from the user, preprocess it in the same way as the documents, and split it into terms.\n",
    "Search: Check which documents contain the query terms by looking up each term in the inverted index.\n",
    "Display Results: Show the documents that match the query terms, or inform the user if no documents were found.\n",
    "This procedure is a basic implementation of a search engine's core functionality, where documents are indexed and queried for relevant results.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
