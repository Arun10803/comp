{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "510b1d5d",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4fcb9027",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd,numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f23f80a-b8cc-4943-a930-f81ca9d0f666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom sklearn.svm import SVC\\nSVC stands for Support Vector Classification, and it's a machine learning model provided by the scikit-learn (sklearn) library.\\nSupport Vector Machine (SVM) is a supervised learning algorithm used for classification tasks (and regression tasks, through SVR).\\nHow it works: SVM finds the hyperplane (or decision boundary) that best separates the data into different classes. The points closest to the hyperplane (called support vectors) are critical to the classifier's accuracy.\\nCommon Usage: SVM is commonly used for binary and multi-class classification tasks, such as text classification, image recognition, and more.\\nfrom sklearn.model_selection import train_test_split\\ntrain_test_split is a function from scikit-learn that splits a dataset into two subsets: one for training the model and one for testing the model. This is essential to evaluate how well your model generalizes to unseen data.\\nWhy is it important?: Splitting data into training and testing sets helps you evaluate the performance of your machine learning model and reduces the risk of overfitting (when a model is too closely fit to the training data and fails to generalize to new data).\\nCommon Usage: You typically split data into training and testing sets in an 80/20 or 70/30 ratio.\\nCountVectorizer is a text feature extraction tool in scikit-learn. It converts a collection of text documents into a matrix of token counts, i.e., it transforms text into numerical features.\\nHow it works: CountVectorizer creates a bag-of-words model. It treats each unique word as a feature and counts the number of occurrences of each word in each document.\\nCommon Usage: This is typically used in text classification problems (e.g., spam detection, sentiment analysis), where you want to represent text as a set of features for a machine learning model.\\nLabelEncoder is used for encoding categorical labels into numerical values. This is particularly useful in machine learning when you need to convert categorical target labels (e.g., 'spam', 'ham' in email classification) into numerical format, which is required by most machine learning algorithms.\\nHow it works: It assigns a unique integer to each label (category) in the target variable. For example, if you have a binary classification task with labels 'spam' and 'ham', LabelEncoder would convert 'spam' to 0 and 'ham' to 1.\\nCommon Usage: Typically used for encoding target labels in classification tasks, but can also be used for encoding categorical features if needed.\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from sklearn.svm import SVC\n",
    "SVC stands for Support Vector Classification, and it's a machine learning model provided by the scikit-learn (sklearn) library.\n",
    "Support Vector Machine (SVM) is a supervised learning algorithm used for classification tasks (and regression tasks, through SVR).\n",
    "How it works: SVM finds the hyperplane (or decision boundary) that best separates the data into different classes. The points closest to the hyperplane (called support vectors) are critical to the classifier's accuracy.\n",
    "Common Usage: SVM is commonly used for binary and multi-class classification tasks, such as text classification, image recognition, and more.\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_test_split is a function from scikit-learn that splits a dataset into two subsets: one for training the model and one for testing the model. This is essential to evaluate how well your model generalizes to unseen data.\n",
    "Why is it important?: Splitting data into training and testing sets helps you evaluate the performance of your machine learning model and reduces the risk of overfitting (when a model is too closely fit to the training data and fails to generalize to new data).\n",
    "Common Usage: You typically split data into training and testing sets in an 80/20 or 70/30 ratio.\n",
    "CountVectorizer is a text feature extraction tool in scikit-learn. It converts a collection of text documents into a matrix of token counts, i.e., it transforms text into numerical features.\n",
    "How it works: CountVectorizer creates a bag-of-words model. It treats each unique word as a feature and counts the number of occurrences of each word in each document.\n",
    "Common Usage: This is typically used in text classification problems (e.g., spam detection, sentiment analysis), where you want to represent text as a set of features for a machine learning model.\n",
    "LabelEncoder is used for encoding categorical labels into numerical values. This is particularly useful in machine learning when you need to convert categorical target labels (e.g., 'spam', 'ham' in email classification) into numerical format, which is required by most machine learning algorithms.\n",
    "How it works: It assigns a unique integer to each label (category) in the target variable. For example, if you have a binary classification task with labels 'spam' and 'ham', LabelEncoder would convert 'spam' to 0 and 'ham' to 1.\n",
    "Common Usage: Typically used for encoding target labels in classification tasks, but can also be used for encoding categorical features if needed.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16f257e5-e04b-463c-92ea-16f5c81afddf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nData Loading:\\n\\nYou would typically use pandas to load a dataset (e.g., a CSV file) into a DataFrame and perform any necessary data preprocessing (cleaning, handling missing values, etc.).\\nFeature Extraction (Text Data):\\n\\nIf your dataset consists of text (like email content or reviews), you would use CountVectorizer to convert the text into numerical features, typically in the form of a document-term matrix (each document is represented as a vector of word counts).\\nLabel Encoding:\\n\\nIf your target labels are categorical (e.g., 'spam', 'ham'), you would use LabelEncoder to convert them into numeric values that can be used by machine learning algorithms.\\nData Splitting:\\n\\nYou would split the dataset into training and test sets using train_test_split to ensure that the model is evaluated on unseen data, helping you assess its performance and generalizability.\\nModeling:\\n\\nWith the training data, you would use SVC (Support Vector Classification) to build a classifier. The model is trained using the features (from CountVectorizer) and the corresponding labels (after encoding).\\nEvaluation:\\n\\nOnce the model is trained, you would use the test data to evaluate how well it performs by comparing the predicted labels (y_pred) with the actual test labels (y_test).\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Data Loading:\n",
    "\n",
    "You would typically use pandas to load a dataset (e.g., a CSV file) into a DataFrame and perform any necessary data preprocessing (cleaning, handling missing values, etc.).\n",
    "Feature Extraction (Text Data):\n",
    "\n",
    "If your dataset consists of text (like email content or reviews), you would use CountVectorizer to convert the text into numerical features, typically in the form of a document-term matrix (each document is represented as a vector of word counts).\n",
    "Label Encoding:\n",
    "\n",
    "If your target labels are categorical (e.g., 'spam', 'ham'), you would use LabelEncoder to convert them into numeric values that can be used by machine learning algorithms.\n",
    "Data Splitting:\n",
    "\n",
    "You would split the dataset into training and test sets using train_test_split to ensure that the model is evaluated on unseen data, helping you assess its performance and generalizability.\n",
    "Modeling:\n",
    "\n",
    "With the training data, you would use SVC (Support Vector Classification) to build a classifier. The model is trained using the features (from CountVectorizer) and the corresponding labels (after encoding).\n",
    "Evaluation:\n",
    "\n",
    "Once the model is trained, you would use the test data to evaluate how well it performs by comparing the predicted labels (y_pred) with the actual test labels (y_test).\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db05214f",
   "metadata": {},
   "source": [
    "# Data importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5c56c00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(r\"C:\\Users\\samay\\OneDrive\\Desktop\\Comp_lab_2\\spam2.csv\",encoding='utf-8',usecols=['v1','v2'])\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39ae8df",
   "metadata": {},
   "source": [
    "# Checking missing values and eliminating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "314a3eba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "v1    0\n",
       "v2    1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea32413f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b82200b",
   "metadata": {},
   "source": [
    "# Converting String to Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a0b982d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "labelenc = LabelEncoder()\n",
    "\n",
    "x = cv.fit_transform(data['v2'])\n",
    "y = labelenc.fit_transform(data['v1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b10c8827-c5ae-4f40-a63c-aed2132934d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. cv = CountVectorizer()\\nCountVectorizer is a feature extraction tool from the scikit-learn library. It is used to convert a collection of text documents into a matrix of token counts, which is commonly known as the Bag-of-Words (BoW) model.\\nWhat it does: It transforms a corpus of text (a collection of documents) into a document-term matrix (DTM), where each row represents a document, and each column represents a unique word (or term) from the entire corpus. The value in each cell of the matrix represents the count of the word in that document.\\nExample: If you have the following two sentences:\\n\\nplaintext\\nCopy code\\n\"I love programming\"\\n\"Programming is fun\"\\nAfter applying CountVectorizer, you would get something like:\\n\\nkotlin\\nCopy code\\n|     | I | love | programming | is | fun |\\n| --- | --- | ---- | ----------- | --- | --- |\\n| 1   | 1   | 1    | 1           | 0   | 0   |\\n| 2   | 0   | 0    | 1           | 1   | 1   |\\nIn this case:\\n\\ncv = CountVectorizer() initializes the CountVectorizer object. This will later be used to fit the text data and transform it into the document-term matrix.\\n\\nfit_transform() method is used to both:\\n\\nFit the vectorizer to the data (i.e., learn the vocabulary from the corpus of text).\\nTransform the text data into a matrix of token counts.\\npython\\nCopy code\\nx = cv.fit_transform(data[\\'v2\\'])\\ndata[\\'v2\\']: This presumably refers to a column in your data DataFrame that contains text documents. So, cv.fit_transform(data[\\'v2\\']) will:\\nLearn the vocabulary from all the text in column v2.\\nConvert the text into a sparse matrix where each row represents a document, and each column represents the count of a specific word in that document.\\nResult: x will be a sparse matrix (or document-term matrix) with the token counts for each word across all documents in the data[\\'v2\\'] column.\\n\\n2. labelenc = LabelEncoder()\\nLabelEncoder is a utility class from the scikit-learn library used for encoding categorical labels into numerical values. It is most commonly used when you have a target variable that consists of labels in text form (e.g., \\'spam\\', \\'ham\\', \\'positive\\', \\'negative\\') and you need to convert those labels into numbers for machine learning algorithms, which usually require numerical inputs.\\nExample: If you have a target variable with labels like:\\n\\nplaintext\\nCopy code\\n[\\'spam\\', \\'ham\\', \\'ham\\', \\'spam\\', \\'spam\\']\\nThe LabelEncoder will convert this into:\\n\\nplaintext\\nCopy code\\n[1, 0, 0, 1, 1]\\nIn your code:\\n\\npython\\nCopy code\\ny = labelenc.fit_transform(data[\\'v1\\'])\\ndata[\\'v1\\']: This refers to a column in your data DataFrame that contains the labels (e.g., \\'spam\\', \\'ham\\', \\'positive\\', \\'negative\\').\\nfit_transform():\\nfit(): Learns the unique categories (or classes) in the column.\\ntransform(): Converts each category (or class) into a corresponding integer label.\\nResult: y will be a numpy array containing the numerical labels corresponding to each label in the data[\\'v1\\'] column.\\n\\nFor example, if data[\\'v1\\'] contained:\\n\\nplaintext\\nCopy code\\n[\\'spam\\', \\'ham\\', \\'ham\\', \\'spam\\', \\'spam\\']\\nAfter applying LabelEncoder(), y will be:\\n\\nplaintext\\nCopy code\\n[1, 0, 0, 1, 1]\\nHere, \\'spam\\' might be encoded as 1, and \\'ham\\' might be encoded as 0.\\n\\nPutting it Together:\\nx = cv.fit_transform(data[\\'v2\\']):\\n\\nThis line processes the text in the data[\\'v2\\'] column (which likely contains text documents) and transforms it into a sparse matrix of token counts (document-term matrix).\\nx will be the feature set for the model, where each row corresponds to a document, and each column represents a word from the vocabulary, with the value representing how many times that word appears in the document.\\ny = labelenc.fit_transform(data[\\'v1\\']):\\n\\nThis line processes the labels in the data[\\'v1\\'] column (which might contain categories like \\'spam\\', \\'ham\\', etc.) and converts them into numerical format so they can be used in machine learning algorithms.\\ny will be the target labels (numerically encoded) that the model will learn to predict.\\nExample Scenario:\\nLet\\'s assume the dataset is related to spam email classification, where:\\n\\ndata[\\'v2\\'] contains the email content.\\ndata[\\'v1\\'] contains the labels (e.g., \\'spam\\' or \\'ham\\').\\ncv.fit_transform(data[\\'v2\\']):\\n\\nThe CountVectorizer learns the vocabulary of the emails in data[\\'v2\\'] and converts the email content into numerical features (the frequency of each word in each email).\\nlabelenc.fit_transform(data[\\'v1\\']):\\n\\nThe LabelEncoder converts the labels (like \\'spam\\' and \\'ham\\') into numbers, such as 1 for \\'spam\\' and 0 for \\'ham\\'.\\nAfter these transformations, the model can use x (the features from the text) and y (the corresponding labels) for training a machine learning model, such as a Support Vector Machine (SVM), Naive Bayes, or any other classifier.\\n\\nFinal Workflow:\\nx contains the transformed text data (as a sparse matrix of word counts).\\ny contains the numerical labels (encoded as 0s and 1s).\\nThis is typically followed by training a classifier (e.g., SVM, logistic regression) to predict the label (y) from the feature set (x).\\n\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "1. cv = CountVectorizer()\n",
    "CountVectorizer is a feature extraction tool from the scikit-learn library. It is used to convert a collection of text documents into a matrix of token counts, which is commonly known as the Bag-of-Words (BoW) model.\n",
    "What it does: It transforms a corpus of text (a collection of documents) into a document-term matrix (DTM), where each row represents a document, and each column represents a unique word (or term) from the entire corpus. The value in each cell of the matrix represents the count of the word in that document.\n",
    "Example: If you have the following two sentences:\n",
    "\n",
    "plaintext\n",
    "Copy code\n",
    "\"I love programming\"\n",
    "\"Programming is fun\"\n",
    "After applying CountVectorizer, you would get something like:\n",
    "\n",
    "kotlin\n",
    "Copy code\n",
    "|     | I | love | programming | is | fun |\n",
    "| --- | --- | ---- | ----------- | --- | --- |\n",
    "| 1   | 1   | 1    | 1           | 0   | 0   |\n",
    "| 2   | 0   | 0    | 1           | 1   | 1   |\n",
    "In this case:\n",
    "\n",
    "cv = CountVectorizer() initializes the CountVectorizer object. This will later be used to fit the text data and transform it into the document-term matrix.\n",
    "\n",
    "fit_transform() method is used to both:\n",
    "\n",
    "Fit the vectorizer to the data (i.e., learn the vocabulary from the corpus of text).\n",
    "Transform the text data into a matrix of token counts.\n",
    "python\n",
    "Copy code\n",
    "x = cv.fit_transform(data['v2'])\n",
    "data['v2']: This presumably refers to a column in your data DataFrame that contains text documents. So, cv.fit_transform(data['v2']) will:\n",
    "Learn the vocabulary from all the text in column v2.\n",
    "Convert the text into a sparse matrix where each row represents a document, and each column represents the count of a specific word in that document.\n",
    "Result: x will be a sparse matrix (or document-term matrix) with the token counts for each word across all documents in the data['v2'] column.\n",
    "\n",
    "2. labelenc = LabelEncoder()\n",
    "LabelEncoder is a utility class from the scikit-learn library used for encoding categorical labels into numerical values. It is most commonly used when you have a target variable that consists of labels in text form (e.g., 'spam', 'ham', 'positive', 'negative') and you need to convert those labels into numbers for machine learning algorithms, which usually require numerical inputs.\n",
    "Example: If you have a target variable with labels like:\n",
    "\n",
    "plaintext\n",
    "Copy code\n",
    "['spam', 'ham', 'ham', 'spam', 'spam']\n",
    "The LabelEncoder will convert this into:\n",
    "\n",
    "plaintext\n",
    "Copy code\n",
    "[1, 0, 0, 1, 1]\n",
    "In your code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "y = labelenc.fit_transform(data['v1'])\n",
    "data['v1']: This refers to a column in your data DataFrame that contains the labels (e.g., 'spam', 'ham', 'positive', 'negative').\n",
    "fit_transform():\n",
    "fit(): Learns the unique categories (or classes) in the column.\n",
    "transform(): Converts each category (or class) into a corresponding integer label.\n",
    "Result: y will be a numpy array containing the numerical labels corresponding to each label in the data['v1'] column.\n",
    "\n",
    "For example, if data['v1'] contained:\n",
    "\n",
    "plaintext\n",
    "Copy code\n",
    "['spam', 'ham', 'ham', 'spam', 'spam']\n",
    "After applying LabelEncoder(), y will be:\n",
    "\n",
    "plaintext\n",
    "Copy code\n",
    "[1, 0, 0, 1, 1]\n",
    "Here, 'spam' might be encoded as 1, and 'ham' might be encoded as 0.\n",
    "\n",
    "Putting it Together:\n",
    "x = cv.fit_transform(data['v2']):\n",
    "\n",
    "This line processes the text in the data['v2'] column (which likely contains text documents) and transforms it into a sparse matrix of token counts (document-term matrix).\n",
    "x will be the feature set for the model, where each row corresponds to a document, and each column represents a word from the vocabulary, with the value representing how many times that word appears in the document.\n",
    "y = labelenc.fit_transform(data['v1']):\n",
    "\n",
    "This line processes the labels in the data['v1'] column (which might contain categories like 'spam', 'ham', etc.) and converts them into numerical format so they can be used in machine learning algorithms.\n",
    "y will be the target labels (numerically encoded) that the model will learn to predict.\n",
    "Example Scenario:\n",
    "Let's assume the dataset is related to spam email classification, where:\n",
    "\n",
    "data['v2'] contains the email content.\n",
    "data['v1'] contains the labels (e.g., 'spam' or 'ham').\n",
    "cv.fit_transform(data['v2']):\n",
    "\n",
    "The CountVectorizer learns the vocabulary of the emails in data['v2'] and converts the email content into numerical features (the frequency of each word in each email).\n",
    "labelenc.fit_transform(data['v1']):\n",
    "\n",
    "The LabelEncoder converts the labels (like 'spam' and 'ham') into numbers, such as 1 for 'spam' and 0 for 'ham'.\n",
    "After these transformations, the model can use x (the features from the text) and y (the corresponding labels) for training a machine learning model, such as a Support Vector Machine (SVM), Naive Bayes, or any other classifier.\n",
    "\n",
    "Final Workflow:\n",
    "x contains the transformed text data (as a sparse matrix of word counts).\n",
    "y contains the numerical labels (encoded as 0s and 1s).\n",
    "This is typically followed by training a classifier (e.g., SVM, logistic regression) to predict the label (y) from the feature set (x).\n",
    "\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfd54a9",
   "metadata": {},
   "source": [
    "# Spliting data into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d3f8942",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x,y, test_size=0.25,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00635ff",
   "metadata": {},
   "source": [
    "# Training a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9cfd985",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;SVC<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.svm.SVC.html\">?<span>Documentation for SVC</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>SVC()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = SVC()\n",
    "svc.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "04b6f187-e4c4-4380-9b33-ff8ccd2958af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nSVC stands for Support Vector Classification. It is a type of Support Vector Machine (SVM) algorithm provided by scikit-learn (a popular machine learning library in Python).\\nSupport Vector Machine (SVM) is a supervised learning algorithm that is widely used for classification and regression tasks. It works by finding the hyperplane that best separates data points of different classes in a higher-dimensional space.\\n\\nSupport Vector Classification specifically refers to the classification task, where the goal is to separate data points belonging to different classes (e.g., spam vs. non-spam emails, cats vs. dogs in images) using a hyperplane.\\n\\nWhen you initialize SVC(), you are creating an instance of the Support Vector Classifier. The SVC() class has several hyperparameters (like the kernel, C, gamma, etc.) that you can customize, but if you don't specify any, it will use the default settings.\\n\\nFor example, the default kernel is 'rbf' (Radial Basis Function), which is commonly used for classification tasks because it can model non-linear decision boundaries.\\nIn this code, we are creating an SVC object called svc that will be used for classification. By default, this will use an RBF kernel.\\n\\n2. svc.fit(X_train, y_train)\\nfit() is a method in scikit-learn that trains a machine learning model using the provided data. In this case, the model being trained is the SVC classifier.\\n\\nX_train: This is the training data (the features) that the model will learn from. In the context of text classification, X_train could be a document-term matrix (or a set of extracted features from text), which represents the text data numerically.\\n\\nEach row in X_train corresponds to a document (an individual data point), and each column corresponds to a specific feature (e.g., the frequency of a particular word, n-gram, or token).\\ny_train: These are the labels or target values corresponding to the training data. These represent the actual categories or classes that you want the model to predict (e.g., 'spam' or 'ham', 'cat' or 'dog').\\n\\nHow fit() works: The fit() method is where the model is actually trained. Here's what happens step-by-step:\\n\\nThe SVC algorithm uses the training data (X_train) and the corresponding labels (y_train) to find the optimal hyperplane (or decision boundary) that best separates the different classes in the feature space.\\n\\nSVM tries to maximize the margin between the support vectors (the closest data points to the hyperplane). The support vectors are the critical data points that are closest to the decision boundary. This helps the model generalize better to unseen data.\\n\\nThe kernel function (by default, RBF) transforms the data into a higher-dimensional space to make it easier to find a linear separating hyperplane, even when the data is non-linearly separable in its original form.\\n\\nThe hyperparameters like C (regularization) and gamma (influence of a single training example) control the tradeoff between fitting the training data well and keeping the model simple to avoid overfitting.\\n\\nIn Summary:\\n\\nX_train is the set of features (data) used to train the model.\\ny_train is the set of labels (or classes) associated with the data.\\nfit() is the method that trains the SVM model by learning from the data and adjusting its internal parameters (such as the support vectors) to best separate the classes.\\n3. Example of how SVC works:\\nHere’s a simplified example of how this might work in practice:\\n\\nSuppose you are building a model to classify emails as either 'spam' or 'ham' (non-spam).\\n\\nPreprocessing: First, you would process the raw email text into a numerical representation, such as a document-term matrix using CountVectorizer or TfidfVectorizer.\\n\\nFeature and Label Extraction:\\n\\nX_train will contain the features extracted from the email text.\\ny_train will contain the labels (0 for 'ham', 1 for 'spam').\\nTraining the Model:\\n\\nWhen you call svc.fit(X_train, y_train), the SVM classifier will analyze the training data and attempt to find the hyperplane (or decision boundary) that best separates the 'spam' and 'ham' emails based on the features in X_train.\\nDecision Boundary:\\n\\nAfter training, the model will be ready to make predictions on new, unseen data by determining which side of the decision boundary the new data points fall on (i.e., which class they belong to).\\n4. Model Prediction:\\nOnce the model has been trained using fit(), you can make predictions on new, unseen data with the predict() method:\\n\\npython\\nCopy code\\ny_pred = svc.predict(X_test)  # Predict the labels for the test data\\nHere, X_test represents the test data (features) that the model has never seen before. The predict() method will classify the test data based on the hyperplane the model learned during training.\\n\\nSummary of the Workflow:\\nInitialize SVC: You create an instance of the SVC classifier.\\n\\npython\\nCopy code\\nsvc = SVC()  # or svc = SVC(kernel='linear') for a linear kernel\\nTrain the Model: You use fit() to train the model on the training data (X_train) and the corresponding labels (y_train).\\n\\npython\\nCopy code\\nsvc.fit(X_train, y_train)\\nModel Trained: The SVM algorithm now has an internal model with a decision boundary (hyperplane) that can separate the data into different classes. This model can then be used to make predictions on new data.\\n\\nKey Points:\\nSVC() creates an SVM classifier.\\n.fit(X_train, y_train) trains the classifier using the training data and labels.\\nThe SVM classifier learns the decision boundary that best separates the different classes in the feature space.\\n\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "SVC stands for Support Vector Classification. It is a type of Support Vector Machine (SVM) algorithm provided by scikit-learn (a popular machine learning library in Python).\n",
    "Support Vector Machine (SVM) is a supervised learning algorithm that is widely used for classification and regression tasks. It works by finding the hyperplane that best separates data points of different classes in a higher-dimensional space.\n",
    "\n",
    "Support Vector Classification specifically refers to the classification task, where the goal is to separate data points belonging to different classes (e.g., spam vs. non-spam emails, cats vs. dogs in images) using a hyperplane.\n",
    "\n",
    "When you initialize SVC(), you are creating an instance of the Support Vector Classifier. The SVC() class has several hyperparameters (like the kernel, C, gamma, etc.) that you can customize, but if you don't specify any, it will use the default settings.\n",
    "\n",
    "For example, the default kernel is 'rbf' (Radial Basis Function), which is commonly used for classification tasks because it can model non-linear decision boundaries.\n",
    "In this code, we are creating an SVC object called svc that will be used for classification. By default, this will use an RBF kernel.\n",
    "\n",
    "2. svc.fit(X_train, y_train)\n",
    "fit() is a method in scikit-learn that trains a machine learning model using the provided data. In this case, the model being trained is the SVC classifier.\n",
    "\n",
    "X_train: This is the training data (the features) that the model will learn from. In the context of text classification, X_train could be a document-term matrix (or a set of extracted features from text), which represents the text data numerically.\n",
    "\n",
    "Each row in X_train corresponds to a document (an individual data point), and each column corresponds to a specific feature (e.g., the frequency of a particular word, n-gram, or token).\n",
    "y_train: These are the labels or target values corresponding to the training data. These represent the actual categories or classes that you want the model to predict (e.g., 'spam' or 'ham', 'cat' or 'dog').\n",
    "\n",
    "How fit() works: The fit() method is where the model is actually trained. Here's what happens step-by-step:\n",
    "\n",
    "The SVC algorithm uses the training data (X_train) and the corresponding labels (y_train) to find the optimal hyperplane (or decision boundary) that best separates the different classes in the feature space.\n",
    "\n",
    "SVM tries to maximize the margin between the support vectors (the closest data points to the hyperplane). The support vectors are the critical data points that are closest to the decision boundary. This helps the model generalize better to unseen data.\n",
    "\n",
    "The kernel function (by default, RBF) transforms the data into a higher-dimensional space to make it easier to find a linear separating hyperplane, even when the data is non-linearly separable in its original form.\n",
    "\n",
    "The hyperparameters like C (regularization) and gamma (influence of a single training example) control the tradeoff between fitting the training data well and keeping the model simple to avoid overfitting.\n",
    "\n",
    "In Summary:\n",
    "\n",
    "X_train is the set of features (data) used to train the model.\n",
    "y_train is the set of labels (or classes) associated with the data.\n",
    "fit() is the method that trains the SVM model by learning from the data and adjusting its internal parameters (such as the support vectors) to best separate the classes.\n",
    "3. Example of how SVC works:\n",
    "Here’s a simplified example of how this might work in practice:\n",
    "\n",
    "Suppose you are building a model to classify emails as either 'spam' or 'ham' (non-spam).\n",
    "\n",
    "Preprocessing: First, you would process the raw email text into a numerical representation, such as a document-term matrix using CountVectorizer or TfidfVectorizer.\n",
    "\n",
    "Feature and Label Extraction:\n",
    "\n",
    "X_train will contain the features extracted from the email text.\n",
    "y_train will contain the labels (0 for 'ham', 1 for 'spam').\n",
    "Training the Model:\n",
    "\n",
    "When you call svc.fit(X_train, y_train), the SVM classifier will analyze the training data and attempt to find the hyperplane (or decision boundary) that best separates the 'spam' and 'ham' emails based on the features in X_train.\n",
    "Decision Boundary:\n",
    "\n",
    "After training, the model will be ready to make predictions on new, unseen data by determining which side of the decision boundary the new data points fall on (i.e., which class they belong to).\n",
    "4. Model Prediction:\n",
    "Once the model has been trained using fit(), you can make predictions on new, unseen data with the predict() method:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "y_pred = svc.predict(X_test)  # Predict the labels for the test data\n",
    "Here, X_test represents the test data (features) that the model has never seen before. The predict() method will classify the test data based on the hyperplane the model learned during training.\n",
    "\n",
    "Summary of the Workflow:\n",
    "Initialize SVC: You create an instance of the SVC classifier.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "svc = SVC()  # or svc = SVC(kernel='linear') for a linear kernel\n",
    "Train the Model: You use fit() to train the model on the training data (X_train) and the corresponding labels (y_train).\n",
    "\n",
    "python\n",
    "Copy code\n",
    "svc.fit(X_train, y_train)\n",
    "Model Trained: The SVM algorithm now has an internal model with a decision boundary (hyperplane) that can separate the data into different classes. This model can then be used to make predictions on new data.\n",
    "\n",
    "Key Points:\n",
    "SVC() creates an SVM classifier.\n",
    ".fit(X_train, y_train) trains the classifier using the training data and labels.\n",
    "The SVM classifier learns the decision boundary that best separates the different classes in the feature space.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9a72433f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9763101220387652"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "37192972-11d3-4bbb-a3e0-33dc4d9c489e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe score() method computes the accuracy of the trained model on the given test data (X_test) and the true labels (y_test).\\nAccuracy is defined as the proportion of correct predictions made by the model on the test set, compared to the total number of predictions.\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "The score() method computes the accuracy of the trained model on the given test data (X_test) and the true labels (y_test).\n",
    "Accuracy is defined as the proportion of correct predictions made by the model on the test set, compared to the total number of predictions.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434f9ee0",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "661d0f68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spam'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email = cv.transform(['Congratulations!!!, You won Lottery of $1500000000 just now, just click on following link https://lottery.com/claim to claim your prize money'])\n",
    "labelenc.classes_[svc.predict(email)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cccf2b81-dbcc-4cb9-8edc-bc504fbef755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nemail = cv.transform([...]):\\n\\ncv.transform([...]): The CountVectorizer (cv) is used to transform the input email text into a numerical feature vector (using the vocabulary learned from the training data).\\nHere, the email text is passed as a list of strings, and the transform method converts the text into a document-term matrix (DTM).\\nResult: email is now a sparse matrix containing the word counts of the email, which can be used by the SVM model.\\nsvc.predict(email):\\n\\nsvc.predict(...): The predict() method is used to make a prediction for the transformed input data (email).\\nThe model (svc) will classify the email based on its learned patterns and return a predicted label (e.g., 0 or 1, depending on the classes it was trained on).\\nResult: The output is an array containing the predicted class label for the email. svc.predict(email)[0] selects the first element of the array, which is the predicted label.\\nlabelenc.classes_[...]:\\n\\nlabelenc.classes_: This is an array of all unique class labels (i.e., the original text labels, such as 'spam' and 'ham') that were encoded into numerical values during training by the LabelEncoder.\\nThe predicted label (e.g., 0 or 1) is used to index into this array, to map the numerical label back to its original class label.\\nResult: It retrieves the human-readable label (e.g., 'spam' or 'ham') based on the predicted number.\\nWhat happens overall:\\nThe email text is transformed into a numerical vector using cv.transform().\\nThe model (svc) predicts the label (e.g., 0 or 1) for the email.\\nThe predicted label is converted back to its original class name (e.g., 'spam' or 'ham') using labelenc.classes_.\\nExample:\\nAssuming the model has been trained to classify emails into 'spam' (encoded as 1) and 'ham' (encoded as 0), the code will output:\\n\\nlabelenc.classes_[1] → 'spam' (if the email is predicted as spam).\\nlabelenc.classes_[0] → 'ham' (if the email is predicted as non-spam).\\n\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "email = cv.transform([...]):\n",
    "\n",
    "cv.transform([...]): The CountVectorizer (cv) is used to transform the input email text into a numerical feature vector (using the vocabulary learned from the training data).\n",
    "Here, the email text is passed as a list of strings, and the transform method converts the text into a document-term matrix (DTM).\n",
    "Result: email is now a sparse matrix containing the word counts of the email, which can be used by the SVM model.\n",
    "svc.predict(email):\n",
    "\n",
    "svc.predict(...): The predict() method is used to make a prediction for the transformed input data (email).\n",
    "The model (svc) will classify the email based on its learned patterns and return a predicted label (e.g., 0 or 1, depending on the classes it was trained on).\n",
    "Result: The output is an array containing the predicted class label for the email. svc.predict(email)[0] selects the first element of the array, which is the predicted label.\n",
    "labelenc.classes_[...]:\n",
    "\n",
    "labelenc.classes_: This is an array of all unique class labels (i.e., the original text labels, such as 'spam' and 'ham') that were encoded into numerical values during training by the LabelEncoder.\n",
    "The predicted label (e.g., 0 or 1) is used to index into this array, to map the numerical label back to its original class label.\n",
    "Result: It retrieves the human-readable label (e.g., 'spam' or 'ham') based on the predicted number.\n",
    "What happens overall:\n",
    "The email text is transformed into a numerical vector using cv.transform().\n",
    "The model (svc) predicts the label (e.g., 0 or 1) for the email.\n",
    "The predicted label is converted back to its original class name (e.g., 'spam' or 'ham') using labelenc.classes_.\n",
    "Example:\n",
    "Assuming the model has been trained to classify emails into 'spam' (encoded as 1) and 'ham' (encoded as 0), the code will output:\n",
    "\n",
    "labelenc.classes_[1] → 'spam' (if the email is predicted as spam).\n",
    "labelenc.classes_[0] → 'ham' (if the email is predicted as non-spam).\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2440e2b-5e38-46b6-ad71-485c8a18de41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf05517-4100-4667-8baf-a43152678d6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cd601f-18a3-41eb-ae72-2a794c3e10c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c69836-8751-4142-92be-e263b329a2c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d115961-349b-452b-b149-39621f203878",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18298878-d7bb-4257-8997-d96bf09ab569",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709a3631-0d01-4723-b5fa-83ec8738b133",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
