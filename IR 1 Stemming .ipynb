{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d5de14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\samay\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "942569b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "\n",
      "Data processing encompasses a series of operations that convert raw data into structured\n",
      "and organized information. This process begins with data collection, where data is gathered\n",
      "from various sources such as sensors, databases, forms, or external systems. Once collected,\n",
      "the data can be in various formats, including text, numbers, images, or multimedia.\n",
      "The next step in data processing is data cleaning and validation. This involves identifying\n",
      "and correcting errors, inconsistencies, and missing values in the data. Clean and accurate data\n",
      "is essential for reliable analysis and decision-making. Data cleaning often involves techniques\n",
      "like outlier detection and data imputation.\n",
      "After data cleaning, data transformation is performed. This includes tasks like data normalization,\n",
      "aggregation, and summarization. Normalization ensures that data is on a consistent scale, while\n",
      "aggregation and summarization reduce data complexity by generating statistics or aggregating data into meaningful groups.\n",
      "Data processing also includes data integration, where data from multiple sources is combined\n",
      "into a unified dataset. Integration can be challenging due to differences in data structures and\n",
      "formats. Techniques like data mapping and data warehousing are used to facilitate integration.\n",
      "\n",
      "\n",
      "Preprocessed Text:\n",
      "data process encompass seri oper convert raw data structur organ inform  process begin data collect  data gather variou sourc sensor  databas  form  extern system  collect  data variou format  includ text  number  imag  multimedia  next step data process data clean valid  involv identifi correct error  inconsist  miss valu data  clean accur data essenti reliabl analysi decision-mak  data clean often involv techniqu like outlier detect data imput  data clean  data transform perform  includ task like data normal  aggreg  summar  normal ensur data consist scale  aggreg summar reduc data complex gener statist aggreg data meaning group  data process also includ data integr  data multipl sourc combin unifi dataset  integr challeng due differ data structur format  techniqu like data map data wareh use facilit integr \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\samay\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# Sample random text (100 words)\n",
    "random_text = \"\"\"\n",
    "Data processing encompasses a series of operations that convert raw data into structured\n",
    "and organized information. This process begins with data collection, where data is gathered\n",
    "from various sources such as sensors, databases, forms, or external systems. Once collected,\n",
    "the data can be in various formats, including text, numbers, images, or multimedia.\n",
    "The next step in data processing is data cleaning and validation. This involves identifying\n",
    "and correcting errors, inconsistencies, and missing values in the data. Clean and accurate data\n",
    "is essential for reliable analysis and decision-making. Data cleaning often involves techniques\n",
    "like outlier detection and data imputation.\n",
    "After data cleaning, data transformation is performed. This includes tasks like data normalization,\n",
    "aggregation, and summarization. Normalization ensures that data is on a consistent scale, while\n",
    "aggregation and summarization reduce data complexity by generating statistics or aggregating data into meaningful groups.\n",
    "Data processing also includes data integration, where data from multiple sources is combined\n",
    "into a unified dataset. Integration can be challenging due to differences in data structures and\n",
    "formats. Techniques like data mapping and data warehousing are used to facilitate integration.\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize the text into words\n",
    "words = word_tokenize(random_text)\n",
    "\n",
    "# Initialize the NLTK Porter Stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Get the English stop words\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Initialize a list to store the preprocessed words\n",
    "preprocessed_words = []\n",
    "\n",
    "# Perform text preprocessing\n",
    "for word in words:\n",
    "    # Remove punctuation and convert to lowercase\n",
    "    word = word.lower()\n",
    "    word = word.strip('.,?!-()[]{}\"')\n",
    "\n",
    "    # Check if the word is not a stop word\n",
    "    if word not in stop_words:\n",
    "        # Stem the word\n",
    "        word = stemmer.stem(word)\n",
    "\n",
    "        # Add the preprocessed word to the list\n",
    "        preprocessed_words.append(word)\n",
    "\n",
    "# Join the preprocessed words back into a text\n",
    "preprocessed_text = \" \".join(preprocessed_words)\n",
    "\n",
    "# Print the original text and preprocessed text\n",
    "print(\"Original Text:\")\n",
    "print(random_text)\n",
    "print(\"\\nPreprocessed Text:\")\n",
    "print(preprocessed_text)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d995bb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The code you've provided is a basic implementation of text preprocessing that involves several key steps, such as tokenization, removing stopwords, stemming, and punctuation removal. Let's go through each part of the code and explain the technical terms and the overall procedure.\n",
    "\n",
    "1. Libraries and Imports\n",
    "NLTK (Natural Language Toolkit): This is a comprehensive Python library for natural language processing (NLP). It provides tools to tokenize text, remove stopwords, stem words, and perform other text processing tasks.\n",
    "word_tokenize: A function that splits a string of text into individual words.\n",
    "stopwords: A corpus of common stopwords (such as \"the\", \"and\", \"is\") in many languages, including English.\n",
    "PorterStemmer: A stemmer from the NLTK library that reduces words to their root form.\n",
    "2. Sample Text (random_text)\n",
    "This is a string containing 100 words, simulating raw, unprocessed text. The text contains multiple sentences related to data processing, cleaning, and transformation. Your goal is to preprocess this text by removing irrelevant parts (like stopwords) and normalizing it (through stemming, lowercasing, etc.).\n",
    "\n",
    "3. Tokenization (word_tokenize)\n",
    "Tokenization is the process of splitting a text into smaller parts (tokens), usually words or sentences. It’s the first step in most NLP tasks because it turns raw text into manageable units.\n",
    "The word_tokenize(random_text) function breaks down the random_text string into individual words. The result is a list of words and punctuation marks, like this:\n",
    "python\n",
    "Copy code\n",
    "['Data', 'processing', 'encompasses', 'a', 'series', 'of', 'operations', ...]\n",
    "4. Stopword Removal\n",
    "Stopwords are common words that don't carry significant meaning and are often removed in text preprocessing. Examples include words like \"the\", \"and\", \"is\", etc. They appear frequently in most texts and do not help much in the analysis.\n",
    "Why remove stopwords?: Removing stopwords helps reduce the size of the dataset, focusing on the more meaningful words for analysis.\n",
    "The NLTK stopwords.words(\"english\") function returns a list of common English stopwords, which you store in stop_words. In the preprocessing loop, you check whether each word in the random_text is a stopword. If it's not, the word is kept; otherwise, it is ignored.\n",
    "5. Punctuation Removal\n",
    "Punctuation Removal: You use the word.strip('.,?!-()[]{}\"') line to remove punctuation marks from words.\n",
    "Why is this needed?: Punctuation marks are generally not useful in text analysis, especially when you want to focus on the meaning of words themselves. For instance, words like \"data.\" and \"data\" should be treated as the same word.\n",
    "This strip method removes common punctuation marks from both ends of a word.\n",
    "For example, the word \"data.\" becomes \"data\", which is easier to process.\n",
    "6. Stemming\n",
    "Stemming is a process that reduces a word to its root or base form. For instance, the stemmer would reduce the word \"processing\" to \"process\", and \"running\" to \"run\".\n",
    "Why stem words?: This reduces variations of a word to a common root, which is useful for text analysis, as it treats different forms of a word as a single unit (e.g., \"run\", \"runs\", \"running\" are all reduced to \"run\").\n",
    "The PorterStemmer().stem(word) method from NLTK is applied to each word after it has been stripped of punctuation and checked against stopwords.\n",
    "7. Preprocessed Words\n",
    "In the loop:\n",
    "Each word is checked to ensure it is not a stopword.\n",
    "The word is then stemmed.\n",
    "The processed word is added to the list preprocessed_words.\n",
    "After processing all words, preprocessed_words will contain a cleaned version of the original text, with stopwords removed and words reduced to their stemmed form.\n",
    "8. Joining Processed Words Back into Text\n",
    "After all the words have been processed, they are joined back into a string using \" \".join(preprocessed_words). This creates a single string, preprocessed_text, where the words are separated by spaces.\n",
    "\n",
    "Example:\n",
    "\n",
    "Original (after tokenization and punctuation removal): ['data', 'process', 'encompass', 'seri', 'oper', 'convert', 'raw', ...]\n",
    "Preprocessed Text: 'data process encompass seri oper convert raw data structur organ inform process ...'\n",
    "9. Output\n",
    "The original text and the preprocessed text are printed for comparison. The preprocessed text will show a cleaner, more uniform version of the text, ready for further analysis like machine learning, text classification, or information retrieval.\n",
    "Summary of the Steps:\n",
    "Tokenize the input text into individual words.\n",
    "Remove stopwords (e.g., \"the\", \"is\", \"a\").\n",
    "Remove punctuation from each word.\n",
    "Stem each word to reduce it to its base form (e.g., \"running\" → \"run\").\n",
    "Reassemble the processed words into a clean text.\n",
    "Why is this important?\n",
    "This preprocessing pipeline is essential for transforming raw text into a format that can be efficiently analyzed. Without these steps, the text would contain unnecessary noise (like stopwords and punctuation) and word variations (like \"running\" and \"runs\") that could hinder the analysis or machine learning models.\n",
    "\n",
    "By preprocessing the text, you ensure that the analysis focuses on the meaningful, relevant parts of the text, making it easier to:\n",
    "\n",
    "Extract insights,\n",
    "Perform sentiment analysis,\n",
    "Build search engines, or\n",
    "Train machine learning models like classifiers or topic models.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
